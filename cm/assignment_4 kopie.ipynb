{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-Learning pt.2  Frozen Lake"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Winter is coming:\n",
    "\n",
    "![](lake.png) \n",
    "Winter is here. You and your friends were tossing around a frisbee at the park when you made a wild throw that left the frisbee out in the middle of the lake. The water is mostly frozen, but there are a few holes where the ice has melted. If you step into one of those holes, you'll fall into the freezing water. At this time, there's an international frisbee shortage, so it's absolutely imperative that you navigate across the lake and retrieve the disc. However, because you don't want to get wet you send out your robot to retrieve the frisbee. The surface is described using a grid like the following:\n",
    "    \n",
    "        State:        State #:\n",
    "        S F F F     0  1  2  3\n",
    "        F H F H     4  5  6  7 \n",
    "        F F F H     8  9  10 11 \n",
    "        H F F G     12 13 14 15 \n",
    "        \n",
    "        \n",
    "    S : starting point, safe\n",
    "    F : frozen surface, safe\n",
    "    H : hole, fall to your doom (instant death)\n",
    "    G : goal, where the frisbee is located\n",
    "    \n",
    "The episode ends when you reach the goal or fall in a hole.\n",
    "    \n",
    "You receive a reward of 1 if you reach the goal, and zero otherwise.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# The Frozenlake is a standard practive environtment from the Open AI Gym. \n",
    "# Gym is a toolkit for developing and comparing reinforcement learning algorithms. \n",
    "# It supports teaching agents everything from walking to playing games like Pong or Pinball.\n",
    "# If you do not have this installed yet (shame, shame): https://gym.openai.com/docs/#installation \n",
    "\n",
    "import gym\n",
    "import math\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### loading the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's load the Frozen lake environment\n",
    "# Slippery means that sometimes the robot will slip on the ice and move in a random direction \n",
    "env_4x4 = gym.make('FrozenLake-v0',map_name=\"4x4\",is_slippery=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### check the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n"
     ]
    }
   ],
   "source": [
    "# This function shows your current location and the environment\n",
    "env_4x4.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement Q-Table learning algorithm\n",
    "\n",
    "Ontwikkel nu een Q learning algoritme dat kan leren hoe het beste van start $S$ naar het doel $G$ te lopen, en dus de wakken de vermijden.\n",
    "\n",
    "Details over Q-learning zijn terug te vinden in de college slides en het hoofdstuk van Gureckis & Love [**computational reinforcement learning**](http://bradlove.org/papers/GureckisLovePress.pdf) en voor meer verdieping in het online boek van [**Sutton & Barto**](http://incompleteideas.net/book/bookdraft2018jan1.pdf) en dan met name hoofdstuk 6.\n",
    "\n",
    "\n",
    "**Let op:** In dit experiment zijn meerdere states. Dit heeft als gevolg dat bij het leren rekening gehouden moet worden met de actie in de volgende state gemaakt wordt. We gaan hier dan vanuit dat de robot altijd de actie kiest met de hoogste Q-value. De prediction error wordt dus:\n",
    "\n",
    "$$\\delta = r_{t+1} + \\gamma\\ max_a\\ Q(s_{t+1} , a) âˆ’ Q(s_t , a_t)$$\n",
    "\n",
    "Waarbij de Q-value update nog steeds is gedefinieerd als:\n",
    "\n",
    "$$Q(s_t, a_t) = Q(s_t, a_t) + \\alpha \\delta$$\n",
    "\n",
    "In het begin van het experiment heeft de robot geen enkele kennis van de wereld en geen enkele verwachtingen voor van het krijgen van beloningen. Voor elke actie op elk vlak van het grid geld dus:\n",
    "\n",
    "$Q(s, LEFT)=Q(s, RIGHT)=Q(s, UP)=Q(s, DOWN)=0$\n",
    "\n",
    "\n",
    "# 1.a (10 punten)\n",
    "\n",
    "Schrijf een functie `q_learn` die als input env (*Open AI Gym* environment), alpha ($\\alpha$), epsilon ($\\epsilon$), gamma ($\\gamma$) en episodes accepteert. \n",
    "\n",
    "De output van deze functie moet een table zijn met $Q$ waarden zijn voor elke state en elke actie. Er zijn 16 states en 4 acties dus in totaal 64 combinaties. Voor de diagnose van het algoritme is het ook belangrijk een lijst met totale verdiende beloningen, maar ook het hoeveelheid stappen tot het doel (per episode) als output te hebben. \n",
    "\n",
    "We gaan er nu van uit dat de robot de $\\epsilon$-greedy keuze regel toepast, waarbij $\\epsilon$ afneemt met tijd. \n",
    "\n",
    "Hier alvast wat code om je op weg te helpen:\n",
    "\n",
    "```python\n",
    "def q_learn(env, alpha=.8, gamma=.95, epsilon=.1, num_episodes=2000):\n",
    "\n",
    "    # Initialize Q table with all zeros\n",
    "    states = env.observation_space.n\n",
    "    actions = env.action_space.n\n",
    "    Q = np.zeros([states,actions])\n",
    "\n",
    "    # Create lists to contain total rewards and steps per episode\n",
    "    jList = [] #step list\n",
    "    rList = [] #reward list\n",
    "\n",
    "    # Start learning:\n",
    "    for i in range(num_episodes):\n",
    "        # Reset environment and get first new observation\n",
    "        s = env.reset()\n",
    "        # Total rewards this episode start at 0\n",
    "        rAll = 0\n",
    "        # You are still not done: (d is specific to frozen lake) \n",
    "        d = False\n",
    "        # Run counter reset:\n",
    "        j = 0\n",
    "    \n",
    "        # The Q-Table learning algorithm, each episode consists of several learning runs (loops)\n",
    "        while j < 100:\n",
    "            j+=1\n",
    "        \n",
    "            # Choose an action by greedily picking from Q table\n",
    "            # NOTE (a)ctions in FrozenLake are 0:left,1:down,2:right,3:up  (this is hardcoded in env.)\n",
    "            # add a tiny tiny bit of noise to the Q table to arbitrate between draws (order of .0001)\n",
    "            # implement the e-greedy rule\n",
    "            a = # TODO\n",
    "            \n",
    "                 \n",
    "            # Get new state and reward from environment, this can/must be done with env.step from the Gym\n",
    "            # where s1 is the new state, r is the reward, and d means the end (hole or frisbee)\n",
    "            s1,r,d,_ = env.step(a)\n",
    "            \n",
    "            # Update Q-Table with new knowledge (assume you pick a with highest value in that state)\n",
    "            Q[s,a] = # TODO\n",
    "            \n",
    "            \n",
    "            # Update total rewards\n",
    "            rAll += r\n",
    "            \n",
    "            # If frisbee or hole (d) then end run\n",
    "            # TODO\n",
    "            \n",
    "            \n",
    "            # Update state\n",
    "            s = s1\n",
    "        \n",
    "   \n",
    "        # Update e, reducing exploration over episodes\n",
    "        epsilon = epsilon*.999\n",
    "        jList.append(j)\n",
    "        rList.append(rAll)\n",
    "    \n",
    "    return (rList, jList, Q)  \n",
    "```\n",
    "\n",
    "de ingevoerde hyperparameters werken goed maar voel je vrij om hier mee te experimenteren. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_learn(env, alpha=.8, gamma=.95, epsilon=.1, num_episodes=2000):\n",
    "\n",
    "    # Initialize Q table with all zeros\n",
    "    states = env.observation_space.n\n",
    "    actions = env.action_space.n\n",
    "    Q = np.zeros([states,actions])\n",
    "\n",
    "    # Create lists to contain total rewards and steps per episode\n",
    "    jList = [] #step list\n",
    "    rList = [] #reward list\n",
    "\n",
    "    # Start learning:\n",
    "    for i in range(num_episodes):\n",
    "        # Reset environment and get first new observation\n",
    "        s = env.reset()\n",
    "        \n",
    "        # Total rewards this episode start at 0\n",
    "        rAll = 0\n",
    "        # You are still not done: (d is specific to frozen lake) \n",
    "        d = False\n",
    "        # Run counter reset:\n",
    "        j = 0\n",
    "    \n",
    "        # The Q-Table learning algorithm, each episode consists of several learning runs (loops)\n",
    "        while j < 100:\n",
    "            j+=1\n",
    "        \n",
    "            # Choose an action by greedily picking from Q table\n",
    "            # NOTE (a)ctions in FrozenLake are 0:left,1:down,2:right,3:up  (this is hardcoded in env.)\n",
    "            # add a tiny tiny bit of noise to the Q table to arbitrate between draws (order of .0001)\n",
    "            # implement the e-greedy rule\n",
    "            \n",
    "            #Noise toevoegen en wat als er twee dezelfde max Q_values zijn?\n",
    "            choice = random.uniform(0, 1)\n",
    "            if choice < epsilon:\n",
    "                a = random.randrange(0,4) \n",
    "            else:\n",
    "                row = Q[s]\n",
    "                a = np.where(row == np.amax(row))\n",
    "                a = np.random.choice(np.reshape(a, len(a[0])))\n",
    "                #a = np.argmax(row)\n",
    "                \n",
    "#                 winner = np.argwhere(row == np.amax(row))\n",
    "#                 winner = np.transpose(winner)\n",
    "#                 flat_list = [item for sublist in winner for item in sublist]\n",
    "#                 a = random.randrange(len(flat_list) - 1)\n",
    "            \n",
    "                 \n",
    "            # Get new state and reward from environment, this can/must be done with env.step from the Gym\n",
    "            # where s1 is the new state, r is the reward, and d means the end (hole or frisbee)\n",
    "            s1,r,d,_ = env.step(a)\n",
    "\n",
    "            # Update Q-Table with new knowledge (assume you pick a with highest value in that state)\n",
    "            row_2 = Q[s1]\n",
    "            max_s1 = np.where(row_2 == np.amax(row_2))\n",
    "            max_s1 = np.random.choice(np.reshape(max_s1, len(max_s1[0])))\n",
    "\n",
    "            delta = r + gamma*(Q[s1][max_s1]) - Q[s][a]\n",
    "            Q[s][a] = Q[s][a] + alpha*delta\n",
    "            \n",
    "            # Update total rewards\n",
    "            rAll += r\n",
    "            \n",
    "            # If frisbee or hole (d) then end run\n",
    "            if d:\n",
    "                break\n",
    "                \n",
    "            \n",
    "            # Update state\n",
    "            s = s1\n",
    "        \n",
    "   \n",
    "        # Update e, reducing exploration over episodes\n",
    "        epsilon = epsilon*.999\n",
    "        jList.append(j)\n",
    "        rList.append(rAll)\n",
    "    \n",
    "    return (rList, jList, Q)  \n",
    "\n",
    "rlist, jlist, Q = q_learn(env_4x4, alpha=.8, gamma=.95, epsilon=.1, num_episodes=2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 1.b   (10 punten) \n",
    "\n",
    "Laat me behulp van deze functie de robot 1000 episodes leren over deze wereld (1 leer episode bestaat dus uit 100 trials). Bereken de gemiddelde score over alle episodes. (heel soms komt dit model niet op de oplossing run het dan nog eens). \n",
    "\n",
    "Plot ook in bins van 10 trials (dus gemiddelde beloning per bin van 10 trials), de verandering in beloningen over tijd. Waar ligt het omslag punt? \n",
    "\n",
    "Plot ook in bins van 10 trails, de hoeveelheid stappen er gemaakt zijn. Hoe kort is de korste route tot succes waar het model op uitkomt? Hoe verhoud zich dit tot de werklijke korste route?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "rlist, jlist, Q = q_learn(env_4x4, alpha=.8, gamma=.95, epsilon=.1, num_episodes=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> De gemiddelde reward score over 1000 episodes is 0.668.\n",
    "\n",
    "> De gemiddelde stappen score over 1000 episodes is 6.56."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1147a2278>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bins = []\n",
    "bins = [np.mean(bin) for bin in np.array_split(rlist, 10)]\n",
    "x = np.linspace(0, 100, len(bins))\n",
    "plt.plot(x, bins)\n",
    "plt.title('Reward list over time')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Het omslagpunt ligt bij 20 trials. Na 20 trials heeft de robot geleerd hoe hij naar de frisbee moet komen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1148a5898>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bins = []\n",
    "bins = [np.mean(bin) for bin in np.array_split(jlist, 10)]\n",
    "x = np.linspace(0, 100, len(bins))\n",
    "plt.plot(x, bins)\n",
    "plt.title('Step list over time')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">De kortste route tot succes uit de grafiek af te lezen is 6.1 stappen. Dit werkelijke kortste route is 6 stappen. \n",
    "Het aantal stappen van het model ligt dus net iets hoger. Dit komt omdat het een gemiddelde is en de robot ook nog af en toe de frisbee niet bereikt. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q tables\n",
    "# 1.c (5 punten)\n",
    "\n",
    "Laten we nu naar de $Q$ tables gaan kijken. We kunnen zo inspecteren welk route de robot zou lopen als deze altijd max$Q$ zou kiezen (dus puur greedy, $epsilon$=0).\n",
    "\n",
    "Gebruik nu de final $Q$ table die `learn_q()` aan het eind geeft. Hierbij is elke kolom de lijst van Q values voor 1 actie in alle 16 states. Dus `output[2][:,0]` geeft je de meest linker kolom en dat zijn de Q values voor actie naar links bewegen. \n",
    "\n",
    "Zorg dat je een lijst krijgt (Qmax) met voor elke state de actie die de hoogste Q value had. Representeer hier elke actie met een getal (0=links, 1=benden, 2=rechts, 3=boven). Plot vervolgens deze lijst in een grid: \n",
    "\n",
    "```python\n",
    "env_4x4.render()\n",
    "print()\n",
    "\n",
    "Qmax=np.asarray(Qmax).reshape((4,4))\n",
    "print(Qmax)\n",
    "\n",
    "plt.matshow(Qmax)\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "Hiervoor gebruiken we [matshow](https://matplotlib.org/api/_as_gen/matplotlib.pyplot.matshow.html), om te zorgen dat de verschillende waarde in de matrix getoond worden met verschillende kleuren. Donkere kleuren zijn hierbij lage waardes en lichte kleuren hogere waardes.\n",
    "\n",
    "Ga nu na hoe de robot looopt, gaat deze links of rechts langs het wak linksboven op de kaart. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Right)\n",
      "SFFF\n",
      "FHF\u001b[41mH\u001b[0m\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "[[2 2 1 0]\n",
      " [3 0 1 0]\n",
      " [0 2 1 0]\n",
      " [0 3 2 0]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x114ac7fd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Qmax = []\n",
    "for row in Q:\n",
    "    row = row.tolist()\n",
    "    Qmax.append(row.index(max(row)))\n",
    "\n",
    "env_4x4.render()\n",
    "print()\n",
    "\n",
    "Qmax=np.asarray(Qmax).reshape((4,4))\n",
    "print(Qmax)\n",
    "\n",
    "plt.matshow(Qmax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Vanuit ons oogpunt gaat de robot rechts langs het wak. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 loading the slippery environment\n",
    "We kijken nu even snel naar hoe de robot zich op glad ijs gaat begeven. Nu zal hij dus af en toe gewoon een andere kant opschuiven. Laad omgeving opnieuw in nu met `is_slippery=True`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's load the Frozen lake environment\n",
    "#slippery means that sometimes the robot will slip on the ice and move in a random direction \n",
    "env_slip = gym.make('FrozenLake-v0',map_name=\"4x4\",is_slippery=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.a (5 punten)\n",
    "Run hetzelfde `q_learn` algoritme, met dezelfde parameter waardes. Laat nu ook weer de gemiddelde score zien, en ook hoe de gemiddelde beloning en het aantal stappen met de tijd verandert. Vergelijk dit met de non-slippery omgeving van hiervoor, noem de meest opvallende verschillen en leg uit waarom we die zien. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 loading the 8x8 environment\n",
    "We gaat het nu wat moeilijker maken en maken het ijsmeer wat groter. Laad nu het **niet gladde** grotere ijsmeer in:\n",
    "```python\n",
    "env = gym.make('FrozenLake-v0',map_name=\"8x8\",is_slippery=False)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's load the Frozen lake environment\n",
    "#slippery means that sometimes you slip on the ice and move in a random direction \n",
    "env_8x8 = gym.make('FrozenLake-v0',map_name=\"8x8\",is_slippery=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check even deze nieuwe omgeving: `env.render()` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function shows your current location and the environment\n",
    "env_8x8.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.a. (5 punten)\n",
    "Het nieuwe meer is 8x8 maar verder helemaal hezelfde. We kunnen dan ook weer hetzelfde `q_learn()` hier op los laten. Doe dit, met weer dezelfde parameter waardes als je eerder bij de 4x4 hebt gebruikt. Rapporteer wederom hoe goed de robot het doet in termen van verkregen beloningen en aantal stappen genomen. Zie je ook een omslag punt?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.b. wereld kennis (20 punten)\n",
    "\n",
    "In het grid van 8x8 zien we dat de robot velen trials nodig heeft om een simpele taak op te lossen. En dit is nog maar een erg simpele wereld met slechts 64 states! We gaan nu proberen of we de Q-learning robot een beetje kunnen helpen sneller te leren. Zoals we in het college gezien hebben (en zie [hier](https://arxiv.org/pdf/1802.10217.pdf)) zijn mensen heel snel in het leren van computer games omdat ze hun kennis over de wereld toepassen op de spelwereld. Laten we dat hier ook proberen en de robot wat meer kennis geven. \n",
    "\n",
    "Om te beginnen is het voor de robot nu niet heel duidelijk dat in een wak vallen een slecht idee is. De beloning die er bij hoort is nu nul. Het is voor mensen duidelijk dat in een wak vallen niet goed is voor het doel behalen. Zo kan je dus een kleine aanpassing maken.\n",
    "Implementeer ook het volgende:\n",
    "\n",
    "```python\n",
    " if d and r == 0:\n",
    "     r = -1\n",
    "```\n",
    "\n",
    "doe dit direct na de `env.step()` stap waarbij dus net `r` is bepaald. Dit zal het model helpen sneller te leren om wakken te vermijden, door daar een negatieve reward aan te geven.  Zorg dat je deze toevoeging ook makkelijk weer aan of uit kunt zetten, zodat je het algoritme met en zonder deze toevoeging kunt testen.\n",
    "\n",
    "Implementeer daarnaast ook nog de volgende twee toevoegingen in het `q_learn` model, die ook makkelijk aan of uit moeten kunnen voor de tests:\n",
    "\n",
    "1.  Leer de robot dat als je in een wak loopt, je ook in dit wak loopt als je van een andere kant op het zelfde hokje loopt. Stel je loop van boven in een wak, leer dan meteen dat je ook in datzelfde wak loopt als je er van links, rechts of van onder in loopt\n",
    "    * *Hint: Houd hierbij rekening met de boundaries, update geen states updaten die niet bestaan of niet in 1 stap bereikbaar zijn.*\n",
    "\n",
    "2. Leer snel af om tegen muren aan te lopen. Nu is deze actie nog geoorloofd en brengt je weer terug op zelfde plek, met een reward van 0. Zorg dat de robot dit niet meer doet.\n",
    "    * *Hint: Leer dit de robot niet af door hier ook een negatieve reward aan te geven, want dan worden de rewards onderling lastiger te vergelijken (gezien er dan meerdere negatieve rewards en 1 positieve reward in een episode kunnen zitten). In plaats daarvan zou je Q-value van de action zelf kunnen aanpassen of aanpassen hoe de action selection precies werkt.*\n",
    "\n",
    "Beide veranderingen kunnen op verschillende manieren worden geimplementeerd. Je kan naar de Q tables kijken om te zien of jouw implementatie gewerkt heeft. \n",
    "\n",
    "Run de verschillende implementaties van het model, met een geen, een of een combinatie van meerdere elementen van kennis. Run elk model 20 keer voor 500 episodes en vergelijk de average rewards. Beschrijf hoe elk element wel of niet bijdraagt aan beter of sneller leren. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.c. (10 punten)\n",
    "Verzin zelf nog een ingreep die het model beter kan laten leren, denk dan vooral na over hoe mensen die spel zouden spelen (common sense). Laat zien hoe veel beter jou model werkt (de beste implementaties zullen we bespreken in het college). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q Networks\n",
    "\n",
    "We hebben nu nog gewerkt met een kleine wereld met maar maximaal 64 states. De echte wereld, en veel leuke spelletjes, hebben natuurlijk een veel grotere state space, en dan wordt het al snel erg lastig om nog een Q table te gaan bijhouden. Dit is waar Q netwerken heel handig zijn, en dan met name Deep Q Networks (DQN). Een deep neural network kan helpen de state space een stuk beter generaliseerbaar te maken, en vergelijkbare Q-values toekennen aan states die veel op elkaar lijken, wat een hele waardevolle eigenschap blijkt (zie bijv. de oorspronkelijke DQN paper over Atari games leren spelen [hier](https://arxiv.org/pdf/1312.5602v1.pdf)).\n",
    "\n",
    "In deze opdracht gaan we nu geen deep Q netwerk in elkaar zetten maar wel een simpel 1-layer Q netwerk, om te leren hoe deze werken en hoe het Q learning algoritme kan worden geÃ¯mplementeerd in een netwerk. We maken hier wel gebruik van [TensorFlow](https://www.tensorflow.org/overview/), een library waarmee je eenvoudig (deep) neural networks kan definiÃ«ren en trainen. Deze library is zeer geschikt voor DQN, en dus ook weer de frozenlake omgeving: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If the ImportError bells of shame are ringing once again:\n",
    "# https://www.tensorflow.org/install/pip\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We gaan een simpel Q netwerk opzetten waarbij er voor elke state (16) een node is in de input laag, en voor elke actie (4) een node in the output laag. Het doel is om de gewichten (weights) van het netwerk zo te trainen dat geven een state als input, de 4 outputs corresponderen met de Q-values van de 4 verschillende acties voor die state. Vervolgens kunnen gewoon een *argmax* gebruiken om de actie met met hoogste Q-value te selecteren. \n",
    "\n",
    "Er zijn 16 input nodes, waarbij elke state nu representeerd kan worden met een [one-hot encoding](https://en.wikipedia.org/wiki/One-hot), dus door alleen de input node van die state op 1 te zetten en alle andere input nodes op 0.\n",
    "\n",
    "De `Qout` laag is gedefineerd als de matrix vermenigvuldiging van inputs en weights, dus simpelweg de som van de inputs vermenigvuldig met de weights voor elke output (deze manier om een neural network te definiÃ«ren zou je bekend moeten voorkomen uit **Leren**). Er is hier dus *geen* activatiefunctie (zoals bijv. Sigmoid of ReLU), i.e. dit is een netwerk met alleen een lineaire layer.\n",
    "\n",
    "We gebruiken hier de Sum Squared Error tussen de berekende Q-values en de eigenlijke Q-values van een state, als de loss functie om het netwerk te trainen en dit is dus ook de functie die geminimaliseerd wordt tijdens het trainen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "states = env_4x4.observation_space.n\n",
    "actions = env_4x4.action_space.n\n",
    "\n",
    "# These lines establish the feed-forward part of the network used to choose actions\n",
    "# A placeholder (tf.placeholder) is simply a variable that we will assign data to at a later date. \n",
    "# It allows us to create our operations and build our computation graph, without needing the data. \n",
    "# In TensorFlow terminology, we then feed data into the graph through these placeholders.\n",
    "inputs1 = tf.placeholder(shape=[1,states],dtype=tf.float32)\n",
    "# Setting the weights to random value\n",
    "W = tf.Variable(tf.random_uniform([states,actions],0,0.01))\n",
    "\n",
    "Qout = tf.matmul(inputs1,W) # we determine the Q values for each action by multiplying inputs by weights\n",
    "predict = tf.argmax(Qout,1) # \"predicted\" action is action with highest Qout (just like Q-learn above)\n",
    "\n",
    "# Below we obtain the loss by taking the sum of squares difference between the target and prediction Q values.\n",
    "nextQ = tf.placeholder(shape=[1,4],dtype=tf.float32)\n",
    "loss = tf.reduce_sum(tf.square(nextQ - Qout)) # we proberen de \n",
    "trainer = tf.train.GradientDescentOptimizer(learning_rate=0.05)\n",
    "updateModel = trainer.minimize(loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nu wordt het tijd om het netwerk te gaan trainen. Hiervoor schrijven we de functie ` q_network `. We helpen je hier een heel stuk op weg. Loop stap voor stap de code door om te kijken of je elke stap begrijpt en vergelijk deze met de stappen van het standaard `q_learn()` algoritme dat je hierboven hebt gebruikt. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_network(env, num_episodes=250, num_rounds=99):\n",
    "    init = tf.initialize_all_variables()\n",
    "    # Set learning parameters\n",
    "    y = .97\n",
    "    e = .3\n",
    "\n",
    "    # Create lists to contain total rewards and steps per episode\n",
    "    jList = []\n",
    "    rList = []\n",
    "    aList = [] # list of actions\n",
    "    maxQnext = [] # list of Q values next state\n",
    "    \n",
    "    # for replay\n",
    "    memory =[]\n",
    "    d_memory =[]\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init)\n",
    "        W_init=(sess.run([W])) # save initial weights\n",
    "        for i in range(num_episodes):\n",
    "            # Reset environment and get first new observation\n",
    "            s = env.reset()\n",
    "            rAll = 0\n",
    "            d = False\n",
    "            j = 0\n",
    "            \n",
    "            # The Q-Network, default run it for 99 rounds per episode\n",
    "            while j < num_rounds:\n",
    "                j+=1\n",
    "            \n",
    "                # Choose an action by greedily (with e chance of random action) from the Q-network\n",
    "                # output a = maxQ of current state [s] and targetQ is list of all Q values in state [s]\n",
    "                a,targetQ = sess.run([predict,Qout],feed_dict={inputs1:np.identity(states)[s:s+1]})\n",
    "           \n",
    "                # e greedy:\n",
    "                if np.random.rand(1) < e:\n",
    "                    a[0] = env.action_space.sample()\n",
    "                aList.append(a[0])\n",
    "                \n",
    "                # Get new state and reward from environment\n",
    "                s1,r,d,_ = env.step(a[0])\n",
    "                \n",
    "                if d and r == 0: # falling in a hole will hurt \n",
    "                    r = -1   \n",
    "            \n",
    "                # Obtain the Q values of the next state by feeding the new state through our network, \n",
    "                # and again assuming you will choose the action with the highest Q value\n",
    "                Q1 = sess.run(Qout,feed_dict={inputs1:np.identity(states)[s1:s1+1]})\n",
    "            \n",
    "                # Obtain maxQ' of next state and use this to update the Q value of our chosen action.\n",
    "                maxQ1 = np.max(Q1)\n",
    "                maxQnext.append(maxQ1)\n",
    "                # In list of targetQ values for s, update the Q value of chosen action\n",
    "                targetQ[0,a[0]] = r + y*maxQ1 \n",
    "                \n",
    "                # Train our network using targetQ list, we try to adjust weights in order to \n",
    "                # minimize prediction error or squared error. \n",
    "                _,W1 = sess.run([updateModel,W],feed_dict={inputs1:np.identity(states)[s:s+1],nextQ:targetQ})\n",
    "                rAll += r\n",
    "                s = s1\n",
    "                \n",
    "                if d:\n",
    "                    break\n",
    "            \n",
    "            # Reduce chance of random action as we train the model.\n",
    "            e = e*.995\n",
    "            jList.append(j)\n",
    "            rList.append(rAll)                \n",
    "                \n",
    "        Weights=(sess.run([W])) # lets save the final weigts!\n",
    "   \n",
    "    print(\"\\nPercent of succesful episodes: \" + str(sum(rList)/num_episodes) + \"%\")\n",
    "    return (rList, jList, aList, W_init, Weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](QNET.png) \n",
    "Hier zie je een schema van hoe het netwerk er uit ziet.\n",
    "\n",
    "# 4.a (5 punten)\n",
    "\n",
    "Run het model voor 1 episode, en 1 ronde. Zo kan je volgen wat er na een stap gebeurd is in het netwerk. Rapporteer de initial weights nog voor de eerste stap gezet wordt, geef ook aan welke actie het model gekozen heeft voor die stap en hoe de nieuwe weights na training er uit zien. Gegeven dat er nu voor maar 1 ronde geleerd wordt, zullen de enige weights die worden aangepast, de weights uit de starting state 0 zijn, dus vergelijk alleen weights die bij deze state horen.\n",
    "\n",
    "Geef aan hoe dit in vergelijking is met de `q_learn()`. Verwijs in je uitleg ook naar prediction errors. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.b (5 punten)\n",
    "\n",
    "Run nu nogmaals het netwerk maar nu met 500 episodes en 99 rounds per episode. Laat net als bij `q_learn` nu zien hoe goed het netwerk is over tijd (rewards & number of steps). Maar laat nu ook zien wat de weights/Q-values zijn voor de vier mogelijke acties in het hokje links naast de frisbee (state 14). Heeft de beste actie ook de hoogste waarde?\n",
    "\n",
    "Gegeven dat states in dit model gepresenteerd worden met een one-hot encoding, en dat het model alleen uit 1 lineaire laag bestaat, worden de Q-values in dit simpele model nog op een vrij directe manier geleerd. Wat is hier dus de relatie tussen de weight matrix en Q-table?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.c Experience Replay (20 punten)\n",
    "\n",
    "Over de tijd zijn de Q_networks op vele manier uitgebreid om nog beter te kunnen leren. Een ding wat we al genoemd hadden was bijvoorbeeld het toevoegen van meerdere lagen. Maar zo is er ook het idee van experience replay, iets wat gebaseerd is op hoe de hersens werken. Wat dit inhoud is dat de robot tussen leer episodes door in zijn geheugen graaft en kijkt wat hij hiervoor gedaan heeft en wat de uitkomst daar van was. Deze herinneringen worden dan weer gebruikt om van te leren (alsof elke herinnering weer een echte gebeurtenis was). Experience replay wordt veel gebruikt in reinforcement learning, de DQN Atari paper gelinked in de section *Q networks* maakt hier bijvoorbeeld ook gebruik van.\n",
    "\n",
    "Wat we hier gaan doen is experience replay toevoegen aan het Q_netwerk. Je hebt hier eigenlijk maar een ding voor nodig; een memory buffer (`memory =[]`)\n",
    "\n",
    "In deze buffer sla voor elke episode van elke run op wat er gebeurde. De staat waar in je was, welke actie je hebt ondernomen, of je een beloning kreeg, welke staat je terecht kwam en of dit het einde van de episode was):   `memory.append((s, a[0], r, s1, d))`\n",
    "\n",
    "Aan het eind van elke episode haal je dan een 30 keer een willekeurige herninnering boven en speelt deze weer uit alsof het echt gebeurde (dus zorgt voor een zelfde update in weights als normaal leren). Bij het begin van een nieuwe leer episode wordt de buffer weer leeggemaakt om ruimte te maken voor nieuwe evaringen. \n",
    "\n",
    "implementeer deze experience replay. Vergelijk de prestatie van het model met en zonder replay, door het beiden modelen op zn minst 10 keer aan te roepen en te kijken naar de verschillen in gemiddelde beloning. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.d Better memories (5 punten)\n",
    "We hebben nu naar replay gekeken waarbij je naar willekeurige herinneringen gaat kijken. Dit voor DQNs al een zeer goede invloed, maar het kan beter. Er zijn verschillende algoritmes bedacht waarbij niet naar willekeurige maar juist naar specifieke herinneringen werdt gekeken op het leren nog verder te optimaliseren. Bedenk en implementeer een vorm van optimalisatie van de replay functie, en test deze ook. Schrijf in je antwoord je motivatie voor je aanpassing (die is belangrijker dan het slagen er van). Moge de beste aanpassing winnen! "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
